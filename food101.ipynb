{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":33884,"sourceType":"datasetVersion","datasetId":1864}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport torchvision\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:20:29.878725Z","iopub.execute_input":"2025-10-23T18:20:29.879257Z","iopub.status.idle":"2025-10-23T18:20:30.951009Z","shell.execute_reply.started":"2025-10-23T18:20:29.879234Z","shell.execute_reply":"2025-10-23T18:20:30.950266Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"np.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:20:33.123815Z","iopub.execute_input":"2025-10-23T18:20:33.124660Z","iopub.status.idle":"2025-10-23T18:20:33.128146Z","shell.execute_reply.started":"2025-10-23T18:20:33.124632Z","shell.execute_reply":"2025-10-23T18:20:33.127346Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"kmader/food41\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:20:34.427231Z","iopub.execute_input":"2025-10-23T18:20:34.427468Z","iopub.status.idle":"2025-10-23T18:20:34.736515Z","shell.execute_reply.started":"2025-10-23T18:20:34.427453Z","shell.execute_reply":"2025-10-23T18:20:34.735897Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/food41\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\n# Load dataset WITHOUT transform first\ndataset_path = os.path.join(path, 'images')\nfull_dataset = datasets.ImageFolder(root=dataset_path, transform=None)  # NO TRANSFORM!\n\n# Split indices\ntotal_size = len(full_dataset)\ntrain_size = int(0.75 * total_size)\nval_size = int(0.15 * total_size)\ntest_size = total_size - train_size - val_size\n\n# Create index splits\nnp.random.seed(42)\nindices = np.random.permutation(total_size)\ntrain_indices = indices[:train_size]\nval_indices = indices[train_size:train_size+val_size]\ntest_indices = indices[train_size+val_size:]\n\n# Create proper datasets with transforms\nclass TransformedSubset(torch.utils.data.Dataset):\n    def __init__(self, dataset, indices, transform=None):\n        self.dataset = dataset\n        self.indices = indices\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.indices)\n    \n    def __getitem__(self, idx):\n        image, label = self.dataset[self.indices[idx]]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# Create datasets\ntrain_dataset = TransformedSubset(full_dataset, train_indices, train_transforms)\nval_dataset = TransformedSubset(full_dataset, val_indices, test_transforms)\ntest_dataset = TransformedSubset(full_dataset, test_indices, test_transforms)\n\n# DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\nprint(f\"Dataset Loaded Successfully!\")\nprint(f\"Train: {train_size:,} | Val: {val_size:,} | Test: {test_size:,}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:20:37.213990Z","iopub.execute_input":"2025-10-23T18:20:37.214565Z","iopub.status.idle":"2025-10-23T18:24:41.311932Z","shell.execute_reply.started":"2025-10-23T18:20:37.214542Z","shell.execute_reply":"2025-10-23T18:24:41.311083Z"}},"outputs":[{"name":"stdout","text":"Dataset Loaded Successfully!\nTrain: 75,750 | Val: 15,150 | Test: 10,100\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Class names\n\nclass_names = full_dataset.classes #classes is a keyword\nnum_classes = len(class_names)\nprint(f\"Total Classes: {num_classes}\")\nprint(f\"Example Classes: {class_names[:10]}\")\n\n\n# Analyze class distribution\ntargets = np.array(full_dataset.targets)\nunique, counts = np.unique(targets, return_counts=True)\nclass_counts = pd.DataFrame({'Class': unique, 'Count': counts})\nclass_counts['Label'] = [class_names[i] for i in unique]\n\nprint(class_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:43:43.553001Z","iopub.execute_input":"2025-10-20T15:43:43.553827Z","iopub.status.idle":"2025-10-20T15:43:43.590874Z","shell.execute_reply.started":"2025-10-20T15:43:43.553801Z","shell.execute_reply":"2025-10-20T15:43:43.590261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\ndef show_batch(loader, n_images=25):\n    dataiter = iter(loader)\n    images, labels = next(dataiter)\n    grid = torchvision.utils.make_grid(images[:n_images], nrow=5, normalize=True)\n    npimg = grid.numpy().transpose((1, 2, 0))\n    plt.figure(figsize=(10, 10))\n    plt.imshow(npimg)\n    plt.axis('off')\n    plt.title(\"Sample Images from Food-101 (Train Split)\")\n    plt.savefig(\"sample_images_grid.png\", dpi=300)\n    plt.close()\n\nshow_batch(train_loader)\nprint(\"✓ Saved sample image grid to 'sample_images_grid.png'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:44:53.575458Z","iopub.execute_input":"2025-10-20T15:44:53.576293Z","iopub.status.idle":"2025-10-20T15:44:57.457033Z","shell.execute_reply.started":"2025-10-20T15:44:53.576262Z","shell.execute_reply":"2025-10-20T15:44:57.456212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check one batch\nimages, labels = next(iter(train_loader))\nprint(\"Batch shape:\", images.shape)\nprint(\"Labels shape:\", labels.shape)\n\n# Show few sample images\ndef imshow(img):\n    img = img * torch.tensor(std).view(3,1,1) + torch.tensor(mean).view(3,1,1)  # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')\n\nplt.figure(figsize=(10, 5))\nimshow(torchvision.utils.make_grid(images[:8]))\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:45:29.310351Z","iopub.execute_input":"2025-10-20T15:45:29.311180Z","iopub.status.idle":"2025-10-20T15:45:30.468535Z","shell.execute_reply.started":"2025-10-20T15:45:29.311150Z","shell.execute_reply":"2025-10-20T15:45:30.467698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # **MODELLING**","metadata":{}},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        \n        # Block 1: 224 -> 112\n        self.conv_block1 = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),  \n        )\n        self.maxpool1 = nn.MaxPool2d(2, 2)\n        \n        # Block 2: 112 -> 56\n        self.conv_block2 = nn.Sequential(\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n        )\n        self.maxpool2 = nn.MaxPool2d(2, 2)\n        \n        # Block 3: 56 -> 28\n        self.conv_block3 = nn.Sequential(\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n        )\n        self.maxpool3 = nn.MaxPool2d(2, 2)\n        \n        # Block 4: 28 -> 14\n        self.conv_block4 = nn.Sequential(\n            nn.Conv2d(256, 512, 3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n        )\n        self.maxpool4 = nn.MaxPool2d(2, 2)\n        \n        # Block 5: 14 -> 7\n        self.conv_block5 = nn.Sequential(\n            nn.Conv2d(512, 512, 3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n        )\n        self.maxpool5 = nn.MaxPool2d(2, 2)\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.5),\n            nn.Linear(512 * 7 * 7, 2048),\n            nn.ReLU(),\n            nn.BatchNorm1d(2048),\n            nn.Dropout(0.5),\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 101)\n        )\n    \n    def forward(self, x):\n        x = self.maxpool1(self.conv_block1(x))\n        x = self.maxpool2(self.conv_block2(x))\n        x = self.maxpool3(self.conv_block3(x))\n        x = self.maxpool4(self.conv_block4(x))\n        x = self.maxpool5(self.conv_block5(x))\n        x = self.classifier(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:33:23.007524Z","iopub.execute_input":"2025-10-20T12:33:23.007849Z","iopub.status.idle":"2025-10-20T12:33:23.017207Z","shell.execute_reply.started":"2025-10-20T12:33:23.007829Z","shell.execute_reply":"2025-10-20T12:33:23.016438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n\n# Scheduler\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=3\n)\n\n# Training function\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in tqdm(loader, desc='Training', leave=False):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # CRITICAL: Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(loader), 100. * correct / total\n\ndef test(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(loader, desc='Validating', leave=False):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(loader), 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:33:23.018000Z","iopub.execute_input":"2025-10-20T12:33:23.018242Z","iopub.status.idle":"2025-10-20T12:33:23.872863Z","shell.execute_reply.started":"2025-10-20T12:33:23.018223Z","shell.execute_reply":"2025-10-20T12:33:23.872047Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Base CNN Model","metadata":{}},{"cell_type":"code","source":"epochs = 20\ntrain_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\nbest_val_acc = 0.0\n\nprint(\"\\nTraining started...\")\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    \n    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = test(model, val_loader, criterion, device)\n    \n    # Update learning rate\n    scheduler.step(test_acc)\n    \n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss: {test_loss:.4f}, Val Acc: {test_acc:.2f}%\")\n    \n    # Save best model\n    if test_acc > best_val_acc:\n        best_val_acc = test_acc\n        torch.save(model.state_dict(), 'best_baseline_model.pth')\n        print(f\"  ✓ Best model saved! Val Acc: {test_acc:.2f}%\")\n    \n    # Early stopping if loss explodes\n    if test_loss > 100:\n        print(\"  ⚠️ Loss exploding! Stopping early.\")\n        break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training Complete!\")\nprint(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T12:33:23.873599Z","iopub.execute_input":"2025-10-20T12:33:23.873804Z","iopub.status.idle":"2025-10-20T15:37:51.968904Z","shell.execute_reply.started":"2025-10-20T12:33:23.873787Z","shell.execute_reply":"2025-10-20T15:37:51.967967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"# Plot training history\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.plot(train_losses, label='Train Loss')\nax1.plot(test_losses, label='Test Loss')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.set_title('Loss over Epochs')\nax1.legend()\nax1.grid(True)\n\nax2.plot(train_accs, label='Train Accuracy')\nax2.plot(test_accs, label='Test Accuracy')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('Accuracy over Epochs')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.savefig('results.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:41:45.980952Z","iopub.execute_input":"2025-10-20T15:41:45.981904Z","iopub.status.idle":"2025-10-20T15:41:46.684604Z","shell.execute_reply.started":"2025-10-20T15:41:45.981875Z","shell.execute_reply":"2025-10-20T15:41:46.683905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nmodel.eval()\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        preds = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n        \n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(preds.cpu().numpy())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:58:55.184461Z","iopub.execute_input":"2025-10-20T15:58:55.185171Z","iopub.status.idle":"2025-10-20T15:59:39.753710Z","shell.execute_reply.started":"2025-10-20T15:58:55.185144Z","shell.execute_reply":"2025-10-20T15:59:39.752861Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_true, y_pred, target_names=class_names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T15:59:52.487407Z","iopub.execute_input":"2025-10-20T15:59:52.487725Z","iopub.status.idle":"2025-10-20T15:59:53.000896Z","shell.execute_reply.started":"2025-10-20T15:59:52.487701Z","shell.execute_reply":"2025-10-20T15:59:53.000143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Top 5 best performing Foods","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n\nper_class_accuracy = []\nfor i, class_name in enumerate(class_names):\n    idx = np.where(y_true == i)[0]\n    acc = np.mean(y_pred[idx] == y_true[idx])\n    per_class_accuracy.append(acc * 100)\n\n# Create a sorted table of top classes\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Class': class_names,\n    'Accuracy': per_class_accuracy,\n    'Sample Size': [np.sum(y_true == i) for i in range(len(class_names))]\n})\ndf = df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\ndf.index += 1\nprint(df.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:00:15.610136Z","iopub.execute_input":"2025-10-20T16:00:15.610686Z","iopub.status.idle":"2025-10-20T16:00:15.630485Z","shell.execute_reply.started":"2025-10-20T16:00:15.610665Z","shell.execute_reply":"2025-10-20T16:00:15.629865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 6))\nplt.bar(df['Class'][:10], df['Accuracy'][:10])\nplt.xticks(rotation=45, ha='right')\nplt.title('Top 10 Best Performing Food-101 Classes')\nplt.ylabel('Accuracy (%)')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:00:30.947678Z","iopub.execute_input":"2025-10-20T16:00:30.948294Z","iopub.status.idle":"2025-10-20T16:00:31.136032Z","shell.execute_reply.started":"2025-10-20T16:00:30.948271Z","shell.execute_reply":"2025-10-20T16:00:31.135328Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Bottom 5 Performing Foods","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame({\n    'Class': class_names,\n    'Accuracy': per_class_accuracy,\n    'Sample Size': [np.sum(y_true == i) for i in range(len(class_names))]\n})\nworst_df = df.sort_values(by='Accuracy', ascending=True).reset_index(drop=True)\nworst_df.index += 1\nprint(worst_df.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:01:15.525609Z","iopub.execute_input":"2025-10-20T16:01:15.525907Z","iopub.status.idle":"2025-10-20T16:01:15.536380Z","shell.execute_reply.started":"2025-10-20T16:01:15.525888Z","shell.execute_reply":"2025-10-20T16:01:15.535687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\n\ncm = confusion_matrix(y_true, y_pred)\nprint(cm.shape)   # should be (101, 101)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Select 10 example classes (e.g., top 10 by accuracy)\ntop10_classes = df.sort_values(by='Accuracy', ascending=False)['Class'].head(10).tolist()\ntop10_idx = [class_names.index(c) for c in top10_classes]\n\n# Subset confusion matrix\ncm_subset = cm[top10_idx][:, top10_idx]\n\n# Normalize\ncm_norm = cm_subset.astype('float') / cm_subset.sum(axis=1)[:, np.newaxis]\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n            xticklabels=top10_classes, yticklabels=top10_classes)\nplt.title(\"Normalized Confusion Matrix (Top 10 Classes)\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:02:21.328051Z","iopub.execute_input":"2025-10-20T16:02:21.328691Z","iopub.status.idle":"2025-10-20T16:02:21.734606Z","shell.execute_reply.started":"2025-10-20T16:02:21.328665Z","shell.execute_reply":"2025-10-20T16:02:21.733841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(25, 22))\nsns.heatmap(cm, cmap=\"Blues\", cbar=False)\nplt.title(\"Food-101 Confusion Matrix (All Classes)\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nnp.save(\"confusion_matrix_food101.npy\", cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T16:03:04.929349Z","iopub.execute_input":"2025-10-20T16:03:04.929954Z","iopub.status.idle":"2025-10-20T16:03:06.414583Z","shell.execute_reply.started":"2025-10-20T16:03:04.929930Z","shell.execute_reply":"2025-10-20T16:03:06.413742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part B: Feature Extraction (Frozen) ","metadata":{}},{"cell_type":"code","source":"# Try this instead:\nfrom torchvision.models import ResNet50_Weights\nres50_model1 = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n#freeze all the layers\nfor param in res50_model1.parameters():\n    param.requires_grad=False\n    \n# Check if weights loaded\nsample_weight = res50_model1.layer1[0].conv1.weight[0,0,0,0].item()\nprint(f\"   Sample weight value: {sample_weight:.6f}\")\nif abs(sample_weight) > 0.5:\n    print(\"   ⚠️ WARNING: Weights look random! Pre-trained might not have loaded.\")\nelse:\n    print(\"   ✓ Pre-trained weights loaded successfully\")\n\n\nnum_features = res50_model1.fc.in_features  # Should be 2048\n\n# Replace classifier (2048 → 512 → 101)\nres50_model1.fc = nn.Linear(2048, 101)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\nres50_model1 = res50_model1.to(device)\ncriterion = nn.CrossEntropyLoss()\n\n# Verify trainable parameters\ntotal_params = sum(p.numel() for p in res50_model1.parameters())\ntrainable_params = sum(p.numel() for p in res50_model1.parameters() if p.requires_grad)\nprint(f\"\\nModel Statistics:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\nprint(f\"  Frozen parameters: {total_params - trainable_params:,}\")\n\noptimizer = optim.SGD(res50_model1.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T13:32:10.347252Z","iopub.execute_input":"2025-10-23T13:32:10.347552Z","iopub.status.idle":"2025-10-23T13:32:10.855263Z","shell.execute_reply.started":"2025-10-23T13:32:10.347522Z","shell.execute_reply":"2025-10-23T13:32:10.854560Z"}},"outputs":[{"name":"stdout","text":"   Sample weight value: 0.003514\n   ✓ Pre-trained weights loaded successfully\nUsing device: cuda\n\nModel Statistics:\n  Total parameters: 23,714,981\n  Trainable parameters: 206,949 (0.87%)\n  Frozen parameters: 23,508,032\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(train_transforms)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T13:18:33.245454Z","iopub.execute_input":"2025-10-23T13:18:33.246038Z","iopub.status.idle":"2025-10-23T13:18:33.249928Z","shell.execute_reply.started":"2025-10-23T13:18:33.246013Z","shell.execute_reply":"2025-10-23T13:18:33.249040Z"}},"outputs":[{"name":"stdout","text":"Compose(\n    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n    RandomHorizontalFlip(p=0.5)\n    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Training function\ndef train(res50_model1, loader, criterion, optimizer, device):\n    res50_model1.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in tqdm(loader, desc='Training', leave=False):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = res50_model1(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # CRITICAL: Gradient clipping\n        #torch.nn.utils.clip_grad_norm_(res50_model1.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(loader), 100. * correct / total\n\ndef test(res50_model1, loader, criterion, device):\n    res50_model1.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(loader, desc='Validating', leave=False):\n            images, labels = images.to(device), labels.to(device)\n            outputs = res50_model1(images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(loader), 100. * correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:24:41.313175Z","iopub.execute_input":"2025-10-23T18:24:41.313389Z","iopub.status.idle":"2025-10-23T18:24:41.320834Z","shell.execute_reply.started":"2025-10-23T18:24:41.313372Z","shell.execute_reply":"2025-10-23T18:24:41.320273Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchvision.models import ResNet50_Weights\n\nprint(\"=\"*80)\nprint(\"COMPLETE DIAGNOSTIC - FINDING THE ISSUE\")\nprint(\"=\"*80)\n\n# ============================================================================\n# TEST 1: VERIFY PRE-TRAINED WEIGHTS ARE ACTUALLY LOADING\n# ============================================================================\n\nprint(\"\\n1. Testing Pre-trained Weights Loading...\")\n\n# Load with explicit weights\nmodel_test = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n# Check multiple weight values (should be specific non-random values)\nw1 = model_test.layer1[0].conv1.weight[0,0,0,0].item()\nw2 = model_test.layer4[2].conv3.weight[0,0,0,0].item()\nw3 = model_test.conv1.weight[0,0,0,0].item()\n\nprint(f\"   Conv1 weight:   {w1:.6f}\")\nprint(f\"   Layer1 weight:  {w2:.6f}\")  \nprint(f\"   Layer4 weight:  {w3:.6f}\")\n\n# These should be specific values like -0.012345, not random\nif abs(w1) > 0.5 or abs(w2) > 0.5 or abs(w3) > 0.5:\n    print(\"   ❌ PROBLEM: Weights look random! Pre-trained not loading!\")\n    print(\"   Try: Download weights manually or check internet connection\")\nelse:\n    print(\"   ✓ Weights look correct\")\n\n# ============================================================================\n# TEST 2: VERIFY DATA TRANSFORMS\n# ============================================================================\n\nprint(\"\\n2. Testing Data Pipeline...\")\n\n# Get one batch\nimages, labels = next(iter(train_loader))\nprint(f\"   Batch shape: {images.shape}\")\nprint(f\"   Batch dtype: {images.dtype}\")\nprint(f\"   Labels shape: {labels.shape}\")\n\n# Check normalization\nimg_mean = images.mean(dim=[0,2,3])\nimg_std = images.std(dim=[0,2,3])\n\nprint(f\"   Image channel means: [{img_mean[0]:.3f}, {img_mean[1]:.3f}, {img_mean[2]:.3f}]\")\nprint(f\"   Image channel stds:  [{img_std[0]:.3f}, {img_std[1]:.3f}, {img_std[2]:.3f}]\")\nprint(f\"   Expected means: ~[0, 0, 0]\")\nprint(f\"   Expected stds:  ~[1, 1, 1]\")\n\nif abs(img_mean[0]) > 0.5:\n    print(\"   ❌ PROBLEM: Images not normalized!\")\nelse:\n    print(\"   ✓ Normalization looks correct\")\n\n# ============================================================================\n# TEST 3: TEST ON IMAGENET IMAGES (SANITY CHECK)\n# ============================================================================\n\nprint(\"\\n3. Testing Model on ImageNet-like Data...\")\n\nmodel_test.eval()\nmodel_test = model_test.to(device)\n\nwith torch.no_grad():\n    # Create random ImageNet-normalized input\n    test_input = torch.randn(8, 3, 224, 224).to(device)\n    test_output = model_test(test_input)\n    \n    # For ImageNet (1000 classes), accuracy on random should be ~0.1%\n    # For our data, pre-trained features should give ~20-30% even with random classifier\n    \n    print(f\"   Output shape: {test_output.shape}\")\n    print(f\"   Output range: [{test_output.min():.2f}, {test_output.max():.2f}]\")\n\n# ============================================================================\n# TEST 4: VERIFY YOUR MODEL SETUP\n# ============================================================================\n\nprint(\"\\n4. Checking Your Model Configuration...\")\n\n# Recreate your exact setup\nres50_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n# Freeze\nfor param in res50_model.parameters():\n    param.requires_grad = False\n\n# Replace FC\nres50_model.fc = nn.Linear(2048, 101)\nres50_model = res50_model.to(device)\n\n# Count parameters\ntotal = sum(p.numel() for p in res50_model.parameters())\ntrainable = sum(p.numel() for p in res50_model.parameters() if p.requires_grad)\nfrozen = total - trainable\n\nprint(f\"   Total params:     {total:,}\")\nprint(f\"   Trainable params: {trainable:,}\")\nprint(f\"   Frozen params:    {frozen:,}\")\n\nif trainable != 206948:  # 2048*101 + 101\n    print(f\"   ❌ PROBLEM: Wrong number of trainable params! Should be 206,948\")\nelse:\n    print(f\"   ✓ Correct number of trainable params\")\n\n# ============================================================================\n# TEST 5: COMPARE WITH BASELINE\n# ============================================================================\n\nprint(\"\\n5. Testing Forward Pass Performance...\")\n\nres50_model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in list(val_loader)[:10]:  # Test on 10 batches\n        images, labels = images.to(device), labels.to(device)\n        outputs = res50_model(images)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\nuntrained_acc = 100. * correct / total\nprint(f\"   Random FC accuracy: {untrained_acc:.2f}%\")\nprint(f\"   Expected: 15-25% (pre-trained features + random classifier)\")\n\nif untrained_acc < 10:\n    print(f\"   ❌ PROBLEM: Too low! Pre-trained features not working!\")\nelif untrained_acc < 20:\n    print(f\"   ⚠️ LOW: Features working but something off\")\nelse:\n    print(f\"   ✓ Pre-trained features working\")\n\n# ============================================================================\n# TEST 6: CHECK LEARNING RATE\n# ============================================================================\n\nprint(\"\\n6. Testing Learning Rate...\")\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(res50_model.fc.parameters(), lr=0.01, momentum=0.9)\n\n# Test one batch\nres50_model.train()\nimages, labels = next(iter(train_loader))\nimages, labels = images.to(device), labels.to(device)\n\n# Before update\nfc_weight_before = res50_model.fc.weight[0,0].item()\n\n# One step\noptimizer.zero_grad()\noutputs = res50_model(images)\nloss = criterion(outputs, labels)\nloss.backward()\noptimizer.step()\n\n# After update  \nfc_weight_after = res50_model.fc.weight[0,0].item()\nweight_change = abs(fc_weight_after - fc_weight_before)\n\nprint(f\"   Weight before: {fc_weight_before:.6f}\")\nprint(f\"   Weight after:  {fc_weight_after:.6f}\")\nprint(f\"   Weight change: {weight_change:.6f}\")\n\nif weight_change < 0.0001:\n    print(f\"   ❌ PROBLEM: Weights barely changing! LR too low or gradient issue\")\nelif weight_change > 0.1:\n    print(f\"   ⚠️ WARNING: Weights changing a lot! LR might be too high\")\nelse:\n    print(f\"   ✓ Weight updates look reasonable\")\n\n# ============================================================================\n# TEST 7: CHECK DATASET BALANCE\n# ============================================================================\n\nprint(\"\\n7. Checking Dataset...\")\n\n# Count labels in training set\nlabel_counts = {}\nfor _, labels in train_loader:\n    for label in labels:\n        label_id = label.item()\n        label_counts[label_id] = label_counts.get(label_id, 0) + 1\n\nunique_classes = len(label_counts)\nmin_samples = min(label_counts.values())\nmax_samples = max(label_counts.values())\n\nprint(f\"   Unique classes: {unique_classes}\")\nprint(f\"   Min samples per class: {min_samples}\")\nprint(f\"   Max samples per class: {max_samples}\")\n\nif unique_classes != 101:\n    print(f\"   ❌ PROBLEM: Should have 101 classes, found {unique_classes}!\")\nelif max_samples / min_samples > 2:\n    print(f\"   ⚠️ WARNING: Imbalanced dataset\")\nelse:\n    print(f\"   ✓ Dataset balance looks good\")\n\n# ============================================================================\n# SUMMARY\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSTIC SUMMARY\")\nprint(\"=\"*80)\n\nproblems_found = []\n\nif abs(w1) > 0.5:\n    problems_found.append(\"Pre-trained weights not loading correctly\")\nif abs(img_mean[0]) > 0.5:\n    problems_found.append(\"Data normalization incorrect\")\nif trainable != 206948:\n    problems_found.append(\"Wrong number of trainable parameters\")\nif untrained_acc < 15:\n    problems_found.append(\"Pre-trained features not working\")\nif weight_change < 0.0001:\n    problems_found.append(\"Learning rate too low or no gradient flow\")\nif unique_classes != 101:\n    problems_found.append(\"Dataset has wrong number of classes\")\n\nif problems_found:\n    print(\"\\n❌ PROBLEMS FOUND:\")\n    for i, problem in enumerate(problems_found, 1):\n        print(f\"   {i}. {problem}\")\nelse:\n    print(\"\\n✅ All checks passed!\")\n    print(\"   Model setup looks correct.\")\n    print(\"   Issue might be:\")\n    print(\"   - Need different learning rate\")\n    print(\"   - Need more epochs\")\n    print(\"   - Dataset difficulty higher than expected\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:38:11.023286Z","iopub.execute_input":"2025-10-23T18:38:11.023919Z","iopub.status.idle":"2025-10-23T18:45:09.484567Z","shell.execute_reply.started":"2025-10-23T18:38:11.023893Z","shell.execute_reply":"2025-10-23T18:45:09.483607Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCOMPLETE DIAGNOSTIC - FINDING THE ISSUE\n================================================================================\n\n1. Testing Pre-trained Weights Loading...\n   Conv1 weight:   0.003514\n   Layer1 weight:  0.004994\n   Layer4 weight:  0.013335\n   ✓ Weights look correct\n\n2. Testing Data Pipeline...\n   Batch shape: torch.Size([32, 3, 224, 224])\n   Batch dtype: torch.float32\n   Labels shape: torch.Size([32])\n   Image channel means: [0.352, -0.030, -0.369]\n   Image channel stds:  [1.250, 1.261, 1.230]\n   Expected means: ~[0, 0, 0]\n   Expected stds:  ~[1, 1, 1]\n   ✓ Normalization looks correct\n\n3. Testing Model on ImageNet-like Data...\n   Output shape: torch.Size([8, 1000])\n   Output range: [-4.66, 8.42]\n\n4. Checking Your Model Configuration...\n   Total params:     23,714,981\n   Trainable params: 206,949\n   Frozen params:    23,508,032\n   ❌ PROBLEM: Wrong number of trainable params! Should be 206,948\n\n5. Testing Forward Pass Performance...\n   Random FC accuracy: 0.31%\n   Expected: 15-25% (pre-trained features + random classifier)\n   ❌ PROBLEM: Too low! Pre-trained features not working!\n\n6. Testing Learning Rate...\n   Weight before: -0.015897\n   Weight after:  -0.015927\n   Weight change: 0.000030\n   ❌ PROBLEM: Weights barely changing! LR too low or gradient issue\n\n7. Checking Dataset...\n   Unique classes: 101\n   Min samples per class: 716\n   Max samples per class: 799\n   ✓ Dataset balance looks good\n\n================================================================================\nDIAGNOSTIC SUMMARY\n================================================================================\n\n❌ PROBLEMS FOUND:\n   1. Wrong number of trainable parameters\n   2. Pre-trained features not working\n   3. Learning rate too low or no gradient flow\n\n================================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from torchvision.models import ResNet50_Weights\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nprint(\"=\"*80)\nprint(\"RESNET50 FEATURE EXTRACTION - CORRECTED VERSION\")\nprint(\"=\"*80)\n\n# Load pre-trained ResNet50\nprint(\"\\n1. Loading ResNet50...\")\nres50_model1 = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n# Check weights loaded\nsample_weight = res50_model1.layer1[0].conv1.weight[0,0,0,0].item()\nprint(f\"   Sample weight: {sample_weight:.6f}\")\nif abs(sample_weight) > 0.5:\n    print(\"   ⚠️ WARNING: Weights look random!\")\nelse:\n    print(\"   ✓ Pre-trained weights loaded\")\n\n# Freeze backbone\nprint(\"\\n2. Freezing backbone...\")\nfor param in res50_model1.parameters():\n    param.requires_grad = False\n\n# Replace classifier\nprint(\"\\n3. Replacing classifier...\")\nnum_features = res50_model1.fc.in_features\nres50_model1.fc = nn.Linear(num_features, 101)\n\n# Move to device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\n4. Moving to device: {device}\")\nres50_model1 = res50_model1.to(device)\n\n# Verify parameters\ntotal_params = sum(p.numel() for p in res50_model1.parameters())\ntrainable_params = sum(p.numel() for p in res50_model1.parameters() if p.requires_grad)\n\nprint(f\"\\n5. Model Statistics:\")\nprint(f\"   Total parameters:     {total_params:,}\")\nprint(f\"   Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\nprint(f\"   Frozen parameters:    {total_params - trainable_params:,}\")\n\n# ============================================================================\n# CRITICAL FIX: ONLY OPTIMIZE FC LAYER!\n# ============================================================================\n\ncriterion = nn.CrossEntropyLoss()\n\n# ✅ CORRECT: Only optimize trainable parameters (FC layer)\noptimizer = optim.SGD(\n    res50_model1.fc.parameters(),  # ← ONLY FC LAYER!\n    lr=0.01, \n    momentum=0.9, \n    weight_decay=1e-4\n)\n\nprint(f\"\\n6. Optimizer configured for FC layer only\")\nprint(f\"   Learning rate: 0.01\")\nprint(f\"   Momentum: 0.9\")\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n)\n\n# ============================================================================\n# TRAINING\n# ============================================================================\n\nepochs = 15\ntrain_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\nbest_val_acc = 0.0\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*80)\n\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    print(\"-\" * 60)\n    \n    train_loss, train_acc = train(res50_model1, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = test(res50_model1, val_loader, criterion, device)\n    \n    scheduler.step(test_acc)\n    \n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss:   {test_loss:.4f},   Val Acc:   {test_acc:.2f}%\")\n    \n    if test_acc > best_val_acc:\n        best_val_acc = test_acc\n        torch.save(res50_model1.state_dict(), 'resnet50_frozen_best.pth')\n        print(f\"  ✓ Best model saved! Val Acc: {test_acc:.2f}%\")\n    \n    # Progress check\n    if epoch == 0:\n        if test_acc < 55:\n            print(f\"  ⚠️ Epoch 1 low ({test_acc:.1f}%). Expected 60-70%\")\n        elif test_acc >= 60:\n            print(f\"  ✓ Epoch 1 looking good!\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TRAINING COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\nprint(f\"Expected: 78-82%\")\nprint(f\"{'='*60}\")\n\n# Final test\nprint(\"\\nEvaluating on test set...\")\nres50_model1.load_state_dict(torch.load('resnet50_frozen_best.pth'))\ntest_loss_final, test_acc_final = test(res50_model1, test_loader, criterion, device)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"FINAL TEST RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Test Accuracy: {test_acc_final:.2f}%\")\nprint(f\"Improvement over baseline: +{test_acc_final - 40.2:.2f}%\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:24:41.321573Z","iopub.execute_input":"2025-10-23T18:24:41.321795Z","iopub.status.idle":"2025-10-23T18:37:45.706141Z","shell.execute_reply.started":"2025-10-23T18:24:41.321775Z","shell.execute_reply":"2025-10-23T18:37:45.705078Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nRESNET50 FEATURE EXTRACTION - CORRECTED VERSION\n================================================================================\n\n1. Loading ResNet50...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 193MB/s] \n","output_type":"stream"},{"name":"stdout","text":"   Sample weight: 0.003514\n   ✓ Pre-trained weights loaded\n\n2. Freezing backbone...\n\n3. Replacing classifier...\n\n4. Moving to device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n5. Model Statistics:\n   Total parameters:     23,714,981\n   Trainable parameters: 206,949 (0.87%)\n   Frozen parameters:    23,508,032\n\n6. Optimizer configured for FC layer only\n   Learning rate: 0.01\n   Momentum: 0.9\n\n================================================================================\nSTARTING TRAINING\n================================================================================\n\nEpoch 1/15\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 2.9148, Train Acc: 32.59%\n  Val Loss:   2.1911,   Val Acc:   47.14%\n  ✓ Best model saved! Val Acc: 47.14%\n  ⚠️ Epoch 1 low (47.1%). Expected 60-70%\n\nEpoch 2/15\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2940658204.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres50_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres50_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2111329151.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(res50_model1, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 15  # Increase to 15 for better convergence\ntrain_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\nbest_val_acc = 0.0\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING RESNET50 - FEATURE EXTRACTION\")\nprint(\"=\"*60)\n\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    print(\"-\" * 60)\n    \n    # Training\n    train_loss, train_acc = train(res50_model1, train_loader, criterion, optimizer, device)\n    \n    # Validation\n    test_loss, test_acc = test(res50_model1, val_loader, criterion, device)\n    \n    # Update learning rate\n    scheduler.step(test_acc)\n    \n    # Save metrics\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    # Print results\n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss:   {test_loss:.4f},   Val Acc:   {test_acc:.2f}%\")\n    \n    # Save best model\n    if test_acc > best_val_acc:\n        best_val_acc = test_acc\n        torch.save(res50_model1.state_dict(), 'resnet50_frozen_best.pth')\n        print(f\"  ✓ Best model saved! Val Acc: {test_acc:.2f}%\")\n    \n    # Early stopping for errors\n    if test_loss > 100 or torch.isnan(torch.tensor(test_loss)):\n        print(\"  ⚠️ Training issue detected! Stopping early.\")\n        break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"FEATURE EXTRACTION TRAINING COMPLETE!\")\nprint(f\"{'='*60}\")\nprint(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\nprint(f\"Expected Range: 78-82%\")\nprint(f\"{'='*60}\")\n\n# ============================================================================\n# STEP 4: FINAL EVALUATION\n# ============================================================================\n\n# Load best model\nprint(\"\\nLoading best model for final evaluation...\")\nres50_model1.load_state_dict(torch.load('resnet50_frozen_best.pth'))\n\n# Evaluate on test set\ntest_loss_final, test_acc_final = test(res50_model1, test_loader, criterion, device)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"FINAL TEST SET RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Test Loss:     {test_loss_final:.4f}\")\nprint(f\"Test Accuracy: {test_acc_final:.2f}%\")\nprint(f\"Improvement over baseline: +{test_acc_final - 40.2:.2f}%\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T13:54:29.781055Z","iopub.execute_input":"2025-10-23T13:54:29.781344Z","iopub.status.idle":"2025-10-23T13:54:29.860029Z","shell.execute_reply.started":"2025-10-23T13:54:29.781324Z","shell.execute_reply":"2025-10-23T13:54:29.859098Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTRAINING RESNET50 - FEATURE EXTRACTION\n============================================================\n\nEpoch 1/15\n------------------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2543897273.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres50_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"],"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nimport numpy as np\n\nprint(\"=\"*80)\nprint(\"RESNET50 FEATURE EXTRACTION - DIAGNOSTIC MODE\")\nprint(\"=\"*80)\n\n# ============================================================================\n# DIAGNOSTIC 1: VERIFY PRE-TRAINED WEIGHTS\n# ============================================================================\n\nprint(\"\\n1. Loading ResNet50...\")\nres50_model1 = models.resnet50(pretrained=True)\n\n# Check if weights loaded\nsample_weight = res50_model1.layer1[0].conv1.weight[0,0,0,0].item()\nprint(f\"   Sample weight value: {sample_weight:.6f}\")\nif abs(sample_weight) > 0.5:\n    print(\"   ⚠️ WARNING: Weights look random! Pre-trained might not have loaded.\")\nelse:\n    print(\"   ✓ Pre-trained weights loaded successfully\")\n\n# ============================================================================\n# DIAGNOSTIC 2: CHECK TRANSFORMS\n# ============================================================================\n\nprint(\"\\n2. Checking data transforms...\")\nprint(f\"   Train transforms: {train_transforms}\")\n\n# Test transform output\nsample_batch, _ = next(iter(train_loader))\nprint(f\"   Batch shape: {sample_batch.shape}\")\nprint(f\"   Batch mean: {sample_batch.mean():.4f} (should be ~0)\")\nprint(f\"   Batch std:  {sample_batch.std():.4f} (should be ~1)\")\nprint(f\"   Batch min:  {sample_batch.min():.4f}\")\nprint(f\"   Batch max:  {sample_batch.max():.4f}\")\n\nif abs(sample_batch.mean()) > 0.5:\n    print(\"   ⚠️ WARNING: Data not normalized correctly!\")\nelse:\n    print(\"   ✓ Data normalization looks correct\")\n\n# ============================================================================\n# DIAGNOSTIC 3: VERIFY ARCHITECTURE\n# ============================================================================\n\nprint(\"\\n3. Setting up model...\")\n\n# Freeze backbone\nfor param in res50_model1.parameters():\n    param.requires_grad = False\n\n# Try SIMPLE classifier first\nprint(\"   Using SIMPLE classifier for testing...\")\nnum_features = res50_model1.fc.in_features\nres50_model1.fc = nn.Linear(num_features, 101)  # Simple version!\n\n# Move to device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nres50_model1 = res50_model1.to(device)\nprint(f\"   Model on device: {next(res50_model1.parameters()).device}\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in res50_model1.parameters())\ntrainable_params = sum(p.numel() for p in res50_model1.parameters() if p.requires_grad)\nfrozen_params = total_params - trainable_params\n\nprint(f\"\\n   Total parameters: {total_params:,}\")\nprint(f\"   Trainable: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\nprint(f\"   Frozen: {frozen_params:,} ({100*frozen_params/total_params:.2f}%)\")\n\nif trainable_params > 1_000_000:\n    print(\"   ⚠️ WARNING: Too many trainable params! Backbone not frozen?\")\nelif trainable_params < 100_000:\n    print(\"   ⚠️ WARNING: Too few trainable params! Classifier issue?\")\nelse:\n    print(\"   ✓ Parameter count looks correct\")\n\n# ============================================================================\n# DIAGNOSTIC 4: TEST FORWARD PASS\n# ============================================================================\n\nprint(\"\\n4. Testing forward pass...\")\nres50_model1.eval()\nwith torch.no_grad():\n    test_input = torch.randn(2, 3, 224, 224).to(device)\n    test_output = res50_model1(test_input)\n    print(f\"   Input shape: {test_input.shape}\")\n    print(f\"   Output shape: {test_output.shape}\")\n    print(f\"   Output range: [{test_output.min():.2f}, {test_output.max():.2f}]\")\n    \n    # Check if output makes sense\n    if test_output.shape != (2, 101):\n        print(\"   ⚠️ WARNING: Wrong output shape!\")\n    else:\n        print(\"   ✓ Forward pass working correctly\")\n\n# ============================================================================\n# DIAGNOSTIC 5: TRAINING SETUP\n# ============================================================================\n\nprint(\"\\n5. Setting up training...\")\ncriterion = nn.CrossEntropyLoss()\n\n# Try HIGHER learning rate\noptimizer = optim.Adam(res50_model1.fc.parameters(), lr=0.003, weight_decay=1e-4)\nprint(f\"   Learning rate: 0.003 (higher than before)\")\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n)\n\n# ============================================================================\n# QUICK 3-EPOCH TEST\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"RUNNING 3-EPOCH DIAGNOSTIC TEST\")\nprint(\"=\"*80)\n\nres50_model1.train()\nfor epoch in range(3):\n    print(f\"\\nEpoch {epoch+1}/3\")\n    print(\"-\" * 60)\n    \n    train_loss, train_acc = train(res50_model1, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = test(res50_model1, val_loader, criterion, device)\n    \n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss:   {test_loss:.4f},   Val Acc:   {test_acc:.2f}%\")\n    \n    # Diagnostic checks\n    if epoch == 0:\n        if test_acc < 50:\n            print(\"  ⚠️ Epoch 1 val accuracy too low! Expected 60-65%\")\n        elif test_acc > 55:\n            print(\"  ✓ Epoch 1 looks good!\")\n        else:\n            print(\"  ⚠️ Epoch 1 slightly low, but training...\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSTIC COMPLETE\")\nprint(\"=\"*80)\n\n# ============================================================================\n# ANALYSIS\n# ============================================================================\n\nprint(\"\\nExpected vs Actual:\")\nprint(f\"  Epoch 1 Val Acc: Expected 60-65%, Got: {test_acc:.2f}%\")\n\nif test_acc >= 60:\n    print(\"\\n✅ GOOD! Model is working correctly.\")\n    print(\"   Continue training to 15 epochs. Should reach 78-82%.\")\nelif test_acc >= 50:\n    print(\"\\n⚠️ BELOW EXPECTED but training.\")\n    print(\"   Possible issues:\")\n    print(\"   - Data quality problems\")\n    print(\"   - Need more epochs\")\n    print(\"   - Learning rate too low\")\nelif test_acc < 50:\n    print(\"\\n❌ PROBLEM DETECTED!\")\n    print(\"   Check the diagnostics above for warnings.\")\n    print(\"   Most likely issues:\")\n    print(\"   1. Pre-trained weights didn't load\")\n    print(\"   2. Data transforms incorrect\")\n    print(\"   3. Backbone accidentally unfrozen\")\n\n# ============================================================================\n# RECOMMENDATIONS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"RECOMMENDATIONS\")\nprint(\"=\"*80)\n\nif test_acc < 55:\n    print(\"\"\"\nIf val accuracy is still low (<55%), try these fixes:\n\n1. RELOAD WITH weights='IMAGENET1K_V1':\n   model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n\n2. USE EVEN SIMPLER CLASSIFIER:\n   model.fc = nn.Linear(2048, 101)\n   # No dropout, no extra layers\n\n3. INCREASE LEARNING RATE:\n   optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n\n4. CHECK YOUR DATA:\n   - Are images actually 224×224?\n   - Is normalization correct?\n   - Any corrupted images?\n\n5. TRY DIFFERENT OPTIMIZER:\n   optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n\"\"\")\nelse:\n    print(\"\"\"\nPerformance is acceptable. To improve:\n\n1. Train full 15 epochs\n2. Switch to complex classifier after epoch 5\n3. Use learning rate scheduling\n4. Consider data augmentation adjustments\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T13:59:11.254327Z","iopub.execute_input":"2025-10-23T13:59:11.254991Z","iopub.status.idle":"2025-10-23T14:26:15.594768Z","shell.execute_reply.started":"2025-10-23T13:59:11.254960Z","shell.execute_reply":"2025-10-23T14:26:15.593900Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nRESNET50 FEATURE EXTRACTION - DIAGNOSTIC MODE\n================================================================================\n\n1. Loading ResNet50...\n   Sample weight value: 0.003514\n   ✓ Pre-trained weights loaded successfully\n\n2. Checking data transforms...\n   Train transforms: Compose(\n    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n    RandomHorizontalFlip(p=0.5)\n    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n   Batch shape: torch.Size([32, 3, 224, 224])\n   Batch mean: -0.1486 (should be ~0)\n   Batch std:  1.2144 (should be ~1)\n   Batch min:  -2.1179\n   Batch max:  2.6400\n   ✓ Data normalization looks correct\n\n3. Setting up model...\n   Using SIMPLE classifier for testing...\n   Model on device: cuda:0\n\n   Total parameters: 23,714,981\n   Trainable: 206,949 (0.87%)\n   Frozen: 23,508,032 (99.13%)\n   ✓ Parameter count looks correct\n\n4. Testing forward pass...\n   Input shape: torch.Size([2, 3, 224, 224])\n   Output shape: torch.Size([2, 101])\n   Output range: [-0.68, 0.97]\n   ✓ Forward pass working correctly\n\n5. Setting up training...\n   Learning rate: 0.003 (higher than before)\n\n================================================================================\nRUNNING 3-EPOCH DIAGNOSTIC TEST\n================================================================================\n\nEpoch 1/3\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 3.6281, Train Acc: 29.48%\n  Val Loss:   2.9186,   Val Acc:   42.20%\n  ⚠️ Epoch 1 val accuracy too low! Expected 60-65%\n\nEpoch 2/3\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 3.4734, Train Acc: 34.36%\n  Val Loss:   3.1465,   Val Acc:   43.15%\n\nEpoch 3/3\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"  Train Loss: 3.4141, Train Acc: 35.89%\n  Val Loss:   2.9685,   Val Acc:   45.85%\n\n================================================================================\nDIAGNOSTIC COMPLETE\n================================================================================\n\nExpected vs Actual:\n  Epoch 1 Val Acc: Expected 60-65%, Got: 45.85%\n\n❌ PROBLEM DETECTED!\n   Check the diagnostics above for warnings.\n   Most likely issues:\n   1. Pre-trained weights didn't load\n   2. Data transforms incorrect\n   3. Backbone accidentally unfrozen\n\n================================================================================\nRECOMMENDATIONS\n================================================================================\n\nIf val accuracy is still low (<55%), try these fixes:\n\n1. RELOAD WITH weights='IMAGENET1K_V1':\n   model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n\n2. USE EVEN SIMPLER CLASSIFIER:\n   model.fc = nn.Linear(2048, 101)\n   # No dropout, no extra layers\n\n3. INCREASE LEARNING RATE:\n   optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n\n4. CHECK YOUR DATA:\n   - Are images actually 224×224?\n   - Is normalization correct?\n   - Any corrupted images?\n\n5. TRY DIFFERENT OPTIMIZER:\n   optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Part C: Feature Extraction (Frozen) ","metadata":{}},{"cell_type":"code","source":"res50_model2=models.resnet50(pretrained=True)\n\n# Replace classifier (2048 → 512 → 101)\nres50_model2.fc= nn.Sequential(\n    nn.Dropout(0.5),\n    nn.Linear(2048,512),\n    nn.ReLU(),\n    nn.BatchNorm1d(512),\n    nn.Dropout(0.5),\n    nn.Linear(512,101)\n)\n\n# Unfreeze layer4 only\nfor param in res50_model2.layer4.parameters():\n    param.requires_grad = True\n\n# Differential learning rates\noptimizer = optim.SGD([\n    {'params': res50_model2.layer4.parameters(), 'lr': 1e-4},  # Backbone\n    {'params': res50_model2.fc.parameters(), 'lr': 1e-3}       # Classifier\n], momentum=0.9, weight_decay=1e-4)\n\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=20\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 15\ntrain_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\nbest_val_acc = 0.0\n\nprint(\"\\nTraining started...\")\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    \n    train_loss, train_acc = train(res50_model2, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = test(res50_model2, val_loader, criterion, device)\n    \n    # Update learning rate\n    scheduler.step(test_acc)\n    \n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss: {test_loss:.4f}, Val Acc: {test_acc:.2f}%\")\n    \n    # Save best model\n    if test_acc > best_val_acc:\n        best_val_acc = test_acc\n        torch.save(res50_model2.state_dict(), 'resnet_unfreeze_best.pth')\n        print(f\"  ✓ Best model saved! Val Acc: {test_acc:.2f}%\")\n    \n    # Early stopping if loss explodes\n    if test_loss > 100:\n        print(\"  ⚠️ Loss exploding! Stopping early.\")\n        break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training Complete!\")\nprint(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part D: Full Fine Tuning","metadata":{}},{"cell_type":"code","source":"res50_model3=models.resnet50(pretrained=True)\n\n# Replace classifier (2048 → 512 → 101)\nres50_model3.fc= nn.Sequential(\n    nn.Dropout(0.5),\n    nn.Linear(2048,512),\n    nn.ReLU(),\n    nn.BatchNorm1d(512),\n    nn.Dropout(0.5),\n    nn.Linear(512,101)\n)\n\n# Unfreeze everything\nfor param in model.parameters():\n    param.requires_grad = True\n\n#Very low LR for stability\noptimizer = optim.SGD(\n    model.parameters(), \n    lr=1e-5,  # Very low!\n    momentum=0.9, \n    weight_decay=1e-4\n)\n\n# Warmup + Cosine decay\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=30\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 20\ntrain_losses, test_losses = [], []\ntrain_accs, test_accs = [], []\nbest_val_acc = 0.0\n\nprint(\"\\nTraining started...\")\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    \n    train_loss, train_acc = train(res50_model3, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = test(res50_model3, val_loader, criterion, device)\n    \n    # Update learning rate\n    scheduler.step(test_acc)\n    \n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    test_losses.append(test_loss)\n    test_accs.append(test_acc)\n    \n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss: {test_loss:.4f}, Val Acc: {test_acc:.2f}%\")\n    \n    # Save best model\n    if test_acc > best_val_acc:\n        best_val_acc = test_acc\n        torch.save(res50_model3.state_dict(), 'resnet_fine_best.pth')\n        print(f\"  ✓ Best model saved! Val Acc: {test_acc:.2f}%\")\n    \n    # Early stopping if loss explodes\n    if test_loss > 100:\n        print(\"  ⚠️ Loss exploding! Stopping early.\")\n        break\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training Complete!\")\nprint(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms, models\nfrom torchvision.models import ResNet50_Weights\nfrom tqdm import tqdm\n\nprint(\"=\"*80)\nprint(\"RESNET50 FEATURE EXTRACTION - FIXED VERSION\")\nprint(\"=\"*80)\n\n# ============================================================================\n# 1. DATA LOADING (USE STANDARD FOOD-101)\n# ============================================================================\n\nprint(\"\\n1. Loading Food-101 dataset...\")\n\n# ImageNet normalization (REQUIRED for pre-trained models)\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Training transforms\ntrain_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\n# Test transforms\ntest_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\n# Load Food-101 using torchvision (CLEAN LOADING)\ntrain_dataset = datasets.Food101(\n    root='./data',\n    split='train',\n    transform=train_transforms,\n    download=True\n)\n\ntest_dataset = datasets.Food101(\n    root='./data',\n    split='test',\n    transform=test_transforms,\n    download=True\n)\n\n# Split train into train + val\ntrain_size = int(0.85 * len(train_dataset))  # 85% train\nval_size = len(train_dataset) - train_size    # 15% val\n\ntrain_subset, val_subset = random_split(\n    train_dataset,\n    [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\nprint(f\"   Train: {len(train_subset):,}\")\nprint(f\"   Val:   {len(val_subset):,}\")\nprint(f\"   Test:  {len(test_dataset):,}\")\n\n# Create loaders\nbatch_size = 32\ntrain_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f\"   Batch size: {batch_size}\")\n\n# ============================================================================\n# 2. VERIFY DATA QUALITY\n# ============================================================================\n\nprint(\"\\n2. Verifying data quality...\")\nsample_images, sample_labels = next(iter(train_loader))\nprint(f\"   Batch shape: {sample_images.shape}\")\nprint(f\"   Image mean: {sample_images.mean():.3f} (expected ~0)\")\nprint(f\"   Image std:  {sample_images.std():.3f} (expected ~1)\")\nprint(f\"   Image min:  {sample_images.min():.3f}\")\nprint(f\"   Image max:  {sample_images.max():.3f}\")\n\n# ============================================================================\n# 3. MODEL SETUP\n# ============================================================================\n\nprint(\"\\n3. Loading ResNet50...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"   Device: {device}\")\n\n# Load pre-trained ResNet50\nmodel = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n# Freeze backbone\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace classifier\nmodel.fc = nn.Linear(2048, 101)\n\n# Move to device\nmodel = model.to(device)\n\n# Verify setup\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"   Total params:     {total_params:,}\")\nprint(f\"   Trainable params: {trainable_params:,}\")\nprint(f\"   Expected:         206,948\")\n\nif trainable_params != 206948:\n    print(f\"   ⚠️ Mismatch! Check model setup\")\n\n# ============================================================================\n# 4. TEST PRE-TRAINED FEATURES\n# ============================================================================\n\nprint(\"\\n4. Testing pre-trained features...\")\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in list(val_loader)[:20]:  # Test 20 batches\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\nrandom_acc = 100. * correct / total\nprint(f\"   Accuracy with random FC: {random_acc:.2f}%\")\nprint(f\"   Expected: 20-30%\")\n\nif random_acc < 15:\n    print(\"   ❌ PROBLEM: Features not working!\")\n    print(\"   This should NOT happen with proper setup\")\nelse:\n    print(\"   ✓ Pre-trained features working!\")\n\n# ============================================================================\n# 5. TRAINING SETUP\n# ============================================================================\n\nprint(\"\\n5. Setting up training...\")\n\ncriterion = nn.CrossEntropyLoss()\n\n# HIGHER learning rate since features aren't learning\noptimizer = optim.SGD(\n    model.fc.parameters(),\n    lr=0.1,  # Much higher!\n    momentum=0.9,\n    weight_decay=1e-4\n)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n)\n\nprint(f\"   Optimizer: SGD\")\nprint(f\"   Learning rate: 0.1 (high for frozen backbone)\")\nprint(f\"   Momentum: 0.9\")\n\n# ============================================================================\n# 6. TRAINING FUNCTIONS\n# ============================================================================\n\ndef train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in tqdm(loader, desc='Training', leave=False):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(loader), 100. * correct / total\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in tqdm(loader, desc='Validating', leave=False):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(loader), 100. * correct / total\n\n# ============================================================================\n# 7. TRAINING\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*80)\n\nepochs = 15\nbest_val_acc = 0.0\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    print(\"-\" * 60)\n    \n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, val_acc = validate(model, val_loader, criterion, device)\n    \n    scheduler.step(val_acc)\n    \n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n    \n    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    print(f\"  Val Loss:   {val_loss:.4f},   Val Acc:   {val_acc:.2f}%\")\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'resnet50_frozen_best.pth')\n        print(f\"  ✓ Best model saved! Val Acc: {val_acc:.2f}%\")\n    \n    # Progress check\n    if epoch == 0:\n        if val_acc < 50:\n            print(f\"  ⚠️ Still low! Expected 60-70%\")\n        elif val_acc >= 60:\n            print(f\"  ✓ Epoch 1 good! On track for 78-82%\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TRAINING COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"Best Val Accuracy: {best_val_acc:.2f}%\")\nprint(f\"Expected: 78-82%\")\nprint(f\"{'='*60}\")\n\n# ============================================================================\n# 8. FINAL EVALUATION\n# ============================================================================\n\nprint(\"\\nFinal evaluation on test set...\")\nmodel.load_state_dict(torch.load('resnet50_frozen_best.pth'))\ntest_loss, test_acc = validate(model, test_loader, criterion, device)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TEST SET RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Test Accuracy: {test_acc:.2f}%\")\nprint(f\"Baseline was: 40.2%\")\nprint(f\"Improvement: +{test_acc - 40.2:.2f}%\")\nprint(f\"{'='*60}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T18:50:34.848801Z","iopub.execute_input":"2025-10-23T18:50:34.849096Z","execution_failed":"2025-10-23T18:50:55.113Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nRESNET50 FEATURE EXTRACTION - FIXED VERSION\n================================================================================\n\n1. Loading Food-101 dataset...\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 471M/5.00G [00:17<02:29, 30.2MB/s]  ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}